{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8C1q6bCwfKhhCBJWtNeZ/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mbatista2004/Imersao-Inteligencia-Artificial/blob/main/Imersao_Alura_Google.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalando o SDK do Google"
      ],
      "metadata": {
        "id": "TlV4-wlhGBTI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oaIy9pFFdo9"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "api.key = userdata.get('SECRET_KEY')\n",
        "genai.configure(api_key=api_key)"
      ],
      "metadata": {
        "id": "tvh2KI_pGiud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Listar os modelos disponiveis"
      ],
      "metadata": {
        "id": "aRXFOoTsGv-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for m in genai.list_models():\n",
        "  if 'generateContent' in m.supported_generation_methods:\n",
        "    print(m.name)"
      ],
      "metadata": {
        "id": "GXZYjzxbGpc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "94d0ce51-9beb-436e-ae83-c91be606bead"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-1.0-pro\n",
            "models/gemini-1.0-pro-001\n",
            "models/gemini-1.0-pro-latest\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-pro\n",
            "models/gemini-pro-vision\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config = {\n",
        "  \"candidate_count\": 1,\n",
        "  \"temperature\": 0.5,\n",
        "}"
      ],
      "metadata": {
        "id": "BWsTO_nHOtCo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "safety_settings={\n",
        "    'HATE': 'BLOCK_NONE',\n",
        "    'HARASSMENT': 'BLOCK_NONE',\n",
        "    'SEXUAL' : 'BLOCK_NONE',\n",
        "    'DANGEROUS' : 'BLOCK_NONE'\n",
        "    }"
      ],
      "metadata": {
        "id": "_sBZgH0MqoGo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(model_name='gemini-1.0-pro',\n",
        "                                  generation_config=generation_config,\n",
        "                                  safety_settings=safety_settings,)"
      ],
      "metadata": {
        "id": "jZ0a7CFIqtrT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\"Que empresa criou o modelo de IA Gemini?\")\n",
        "response.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "dO37SZnlqz4T",
        "outputId": "503b1ff3-3590-4f36-914e-28d1fde75b6e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Google'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat = model.start_chat(history=[])\n",
        "\n",
        "prompt = input('Esperando prompt: ')\n",
        "\n",
        "while prompt != \"fim\":\n",
        "  response = chat.send_message(prompt)\n",
        "  print(\"Resposta:\", response.text, '\\n\\n')\n",
        "  prompt = input('Esperando prompt: ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qoZSc0VGq6sm",
        "outputId": "455655b6-f8d0-48f2-f244-08b6483fb433"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Esperando prompt: Fim\n",
            "Resposta: **Fim**\n",
            "\n",
            "A jornada chegou ao fim,\n",
            "As cortinas se fecham,\n",
            "O espetáculo acabou,\n",
            "Os aplausos ecoam.\n",
            "\n",
            "Os personagens se despedem,\n",
            "Suas histórias contadas,\n",
            "As lições aprendidas,\n",
            "As memórias guardadas.\n",
            "\n",
            "O palco fica vazio,\n",
            "As luzes se apagam,\n",
            "O silêncio envolve,\n",
            "O fim se aproxima.\n",
            "\n",
            "Mas o legado permanece,\n",
            "Nas palavras escritas,\n",
            "Nas emoções evocadas,\n",
            "Nas vidas tocadas.\n",
            "\n",
            "Pois mesmo quando a cortina cai,\n",
            "A história continua,\n",
            "Nos corações e mentes,\n",
            "Para sempre viva.\n",
            "\n",
            "Então, quando o fim chegar,\n",
            "Não chore a despedida,\n",
            "Celebre a jornada,\n",
            "E a magia que ela trouxe.\n",
            "\n",
            "Pois o fim é apenas um novo começo,\n",
            "Uma nova aventura para começar,\n",
            "Uma nova história para ser escrita,\n",
            "Um novo capítulo para ser vivido. \n",
            "\n",
            "\n",
            "Esperando prompt: Fim\n",
            "Resposta: **Fim**\n",
            "\n",
            "A jornada termina aqui,\n",
            "O capítulo final é lido,\n",
            "As cortinas se fecham,\n",
            "A história é concluída.\n",
            "\n",
            "Os personagens se despedem,\n",
            "Suas histórias contadas,\n",
            "As lições aprendidas,\n",
            "As memórias guardadas.\n",
            "\n",
            "O palco fica vazio,\n",
            "As luzes se apagam,\n",
            "O silêncio envolve,\n",
            "O fim se aproxima.\n",
            "\n",
            "Mas o legado permanece,\n",
            "Nas palavras escritas,\n",
            "Nas emoções evocadas,\n",
            "Nas vidas tocadas.\n",
            "\n",
            "Pois mesmo quando a cortina cai,\n",
            "A história continua,\n",
            "Nos corações e mentes,\n",
            "Para sempre viva.\n",
            "\n",
            "Então, quando o fim chegar,\n",
            "Não chore a despedida,\n",
            "Celebre a jornada,\n",
            "E a magia que ela trouxe.\n",
            "\n",
            "Pois o fim é apenas um novo começo,\n",
            "Uma nova aventura para começar,\n",
            "Uma nova história para ser escrita,\n",
            "Um novo capítulo para ser vivido.\n",
            "\n",
            "Mas por enquanto, deixemos a cortina cair,\n",
            "E agradeçamos pela história que foi compartilhada,\n",
            "Pelas lições que aprendemos,\n",
            "E pelas memórias que fizemos.\n",
            "\n",
            "**Fim** \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-481ebd21a3b4>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Resposta:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Esperando prompt: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvM0G0_cref8",
        "outputId": "89a2007a-c79d-4ff2-b38b-842ba84e3418"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatSession(\n",
              "    model=genai.GenerativeModel(\n",
              "        model_name='models/gemini-1.0-pro',\n",
              "        generation_config={'candidate_count': 1, 'temperature': 0.5},\n",
              "        safety_settings={<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 8>: <HarmBlockThreshold.BLOCK_NONE: 4>, <HarmCategory.HARM_CATEGORY_HARASSMENT: 7>: <HarmBlockThreshold.BLOCK_NONE: 4>, <HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 9>: <HarmBlockThreshold.BLOCK_NONE: 4>, <HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 10>: <HarmBlockThreshold.BLOCK_NONE: 4>},\n",
              "        tools=None,\n",
              "        system_instruction=None,\n",
              "    ),\n",
              "    history=[glm.Content({'parts': [{'text': 'Fim'}], 'role': 'user'}), glm.Content({'parts': [{'text': '**Fim**\\n\\nA...a ser vivido.'}], 'role': 'model'}), glm.Content({'parts': [{'text': 'Fim'}], 'role': 'user'}), glm.Content({'parts': [{'text': '**Fim**\\n\\nA...s.\\n\\n**Fim**'}], 'role': 'model'})]\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat.history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSuJX8pCrkId",
        "outputId": "bd0d314f-04ed-4d3c-cf11-ab2de3c21d9c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[parts {\n",
              "   text: \"Fim\"\n",
              " }\n",
              " role: \"user\",\n",
              " parts {\n",
              "   text: \"**Fim**\\n\\nA jornada chegou ao fim,\\nAs cortinas se fecham,\\nO espet\\303\\241culo acabou,\\nOs aplausos ecoam.\\n\\nOs personagens se despedem,\\nSuas hist\\303\\263rias contadas,\\nAs li\\303\\247\\303\\265es aprendidas,\\nAs mem\\303\\263rias guardadas.\\n\\nO palco fica vazio,\\nAs luzes se apagam,\\nO sil\\303\\252ncio envolve,\\nO fim se aproxima.\\n\\nMas o legado permanece,\\nNas palavras escritas,\\nNas emo\\303\\247\\303\\265es evocadas,\\nNas vidas tocadas.\\n\\nPois mesmo quando a cortina cai,\\nA hist\\303\\263ria continua,\\nNos cora\\303\\247\\303\\265es e mentes,\\nPara sempre viva.\\n\\nEnt\\303\\243o, quando o fim chegar,\\nN\\303\\243o chore a despedida,\\nCelebre a jornada,\\nE a magia que ela trouxe.\\n\\nPois o fim \\303\\251 apenas um novo come\\303\\247o,\\nUma nova aventura para come\\303\\247ar,\\nUma nova hist\\303\\263ria para ser escrita,\\nUm novo cap\\303\\255tulo para ser vivido.\"\n",
              " }\n",
              " role: \"model\",\n",
              " parts {\n",
              "   text: \"Fim\"\n",
              " }\n",
              " role: \"user\",\n",
              " parts {\n",
              "   text: \"**Fim**\\n\\nA jornada termina aqui,\\nO cap\\303\\255tulo final \\303\\251 lido,\\nAs cortinas se fecham,\\nA hist\\303\\263ria \\303\\251 conclu\\303\\255da.\\n\\nOs personagens se despedem,\\nSuas hist\\303\\263rias contadas,\\nAs li\\303\\247\\303\\265es aprendidas,\\nAs mem\\303\\263rias guardadas.\\n\\nO palco fica vazio,\\nAs luzes se apagam,\\nO sil\\303\\252ncio envolve,\\nO fim se aproxima.\\n\\nMas o legado permanece,\\nNas palavras escritas,\\nNas emo\\303\\247\\303\\265es evocadas,\\nNas vidas tocadas.\\n\\nPois mesmo quando a cortina cai,\\nA hist\\303\\263ria continua,\\nNos cora\\303\\247\\303\\265es e mentes,\\nPara sempre viva.\\n\\nEnt\\303\\243o, quando o fim chegar,\\nN\\303\\243o chore a despedida,\\nCelebre a jornada,\\nE a magia que ela trouxe.\\n\\nPois o fim \\303\\251 apenas um novo come\\303\\247o,\\nUma nova aventura para come\\303\\247ar,\\nUma nova hist\\303\\263ria para ser escrita,\\nUm novo cap\\303\\255tulo para ser vivido.\\n\\nMas por enquanto, deixemos a cortina cair,\\nE agrade\\303\\247amos pela hist\\303\\263ria que foi compartilhada,\\nPelas li\\303\\247\\303\\265es que aprendemos,\\nE pelas mem\\303\\263rias que fizemos.\\n\\n**Fim**\"\n",
              " }\n",
              " role: \"model\"]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Melhorando a visualização\n",
        "#Código disponível em https://ai.google.dev/tutorials/python_quickstart#import_packages\n",
        "import textwrap\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
        "\n",
        "#Imprimindo o histórico\n",
        "for message in chat.history:\n",
        "  display(to_markdown(f'**{message.role}**: {message.parts[0].text}'))\n",
        "  print('-------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "id": "0jjnZHwLrm4Q",
        "outputId": "590b2376-4c11-4f2c-9367-82281c87e351"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> **user**: Fim"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> **model**: **Fim**\n> \n> A jornada chegou ao fim,\n> As cortinas se fecham,\n> O espetáculo acabou,\n> Os aplausos ecoam.\n> \n> Os personagens se despedem,\n> Suas histórias contadas,\n> As lições aprendidas,\n> As memórias guardadas.\n> \n> O palco fica vazio,\n> As luzes se apagam,\n> O silêncio envolve,\n> O fim se aproxima.\n> \n> Mas o legado permanece,\n> Nas palavras escritas,\n> Nas emoções evocadas,\n> Nas vidas tocadas.\n> \n> Pois mesmo quando a cortina cai,\n> A história continua,\n> Nos corações e mentes,\n> Para sempre viva.\n> \n> Então, quando o fim chegar,\n> Não chore a despedida,\n> Celebre a jornada,\n> E a magia que ela trouxe.\n> \n> Pois o fim é apenas um novo começo,\n> Uma nova aventura para começar,\n> Uma nova história para ser escrita,\n> Um novo capítulo para ser vivido."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> **user**: Fim"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> **model**: **Fim**\n> \n> A jornada termina aqui,\n> O capítulo final é lido,\n> As cortinas se fecham,\n> A história é concluída.\n> \n> Os personagens se despedem,\n> Suas histórias contadas,\n> As lições aprendidas,\n> As memórias guardadas.\n> \n> O palco fica vazio,\n> As luzes se apagam,\n> O silêncio envolve,\n> O fim se aproxima.\n> \n> Mas o legado permanece,\n> Nas palavras escritas,\n> Nas emoções evocadas,\n> Nas vidas tocadas.\n> \n> Pois mesmo quando a cortina cai,\n> A história continua,\n> Nos corações e mentes,\n> Para sempre viva.\n> \n> Então, quando o fim chegar,\n> Não chore a despedida,\n> Celebre a jornada,\n> E a magia que ela trouxe.\n> \n> Pois o fim é apenas um novo começo,\n> Uma nova aventura para começar,\n> Uma nova história para ser escrita,\n> Um novo capítulo para ser vivido.\n> \n> Mas por enquanto, deixemos a cortina cair,\n> E agradeçamos pela história que foi compartilhada,\n> Pelas lições que aprendemos,\n> E pelas memórias que fizemos.\n> \n> **Fim**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}